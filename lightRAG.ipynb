{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Website Crawling and Data Extraction\n",
    "\n",
    "In this section, we use `crawl4ai` to crawl one or more websites. The extracted text is saved as `.md` files in a folder (e.g., `./crawled_data`).\n",
    "\n",
    "You can enter the URLs interactively using the provided widget.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you run into issues with the event loop in Jupyter Notebook, uncomment the next two lines:\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Imports used for Crawl4AI part\n",
    "import asyncio\n",
    "import threading\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from typing import Set, Any, List\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "from urllib.parse import urlparse\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "# Import Crawl4AI classes based on the documentation\n",
    "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set Windows Proactor Event Loop (for Windows users) ---\n",
    "import sys\n",
    "if sys.platform == \"win32\":\n",
    "    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the following cell and the use the widget to set the urls for the crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d996e448464433af0e0b0caea9f427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='https://crawl4ai.com/mkdocs/\\nhttps://stable-learn.com/en/lightrag-introduction/', description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- URL Input Widget ---\n",
    "url_input = widgets.Textarea(\n",
    "    value='https://crawl4ai.com/mkdocs/\\nhttps://stable-learn.com/en/lightrag-introduction/',\n",
    "    placeholder='Enter one URL per line',\n",
    "    description='URLs:',\n",
    "    layout=widgets.Layout(width='100%', height='80px')\n",
    ")\n",
    "display(url_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_safe_filename(url: str, extension: str = \".md\") -> str:\n",
    "    \"\"\"\n",
    "    Generate a safe and unique filename from a URL.\n",
    "    \n",
    "    The filename is based on the domain, path, and (if present) fragment of the URL.\n",
    "    A short hash of the full URL is appended to ensure uniqueness.\n",
    "    \n",
    "    Examples:\n",
    "      - \"https://crawl4ai.com/\" becomes something like \"crawl4ai.com_index_<hash>.md\"\n",
    "      - \"https://crawl4ai.com/#quick-start\" becomes \"crawl4ai.com_index_quick_start_<hash>.md\"\n",
    "      \n",
    "      \n",
    "    Params:\n",
    "        url: The URL to generate a filename for.\n",
    "        extension: The file extension to use (default: \".md\").\n",
    "        \n",
    "    Returns:\n",
    "        A safe and unique filename based on the URL.\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    \n",
    "    # Get the netloc and sanitize it (replace ':' with '_' if needed)\n",
    "    netloc = parsed.netloc.replace(\":\", \"_\")\n",
    "    \n",
    "    # Use the path; default to \"index\" if empty\n",
    "    path = parsed.path.strip(\"/\")\n",
    "    if not path:\n",
    "        path = \"index\"\n",
    "    \n",
    "    # Use the fragment if present (replace any non-alphanumeric characters with underscores)\n",
    "    fragment = parsed.fragment.strip()\n",
    "    if fragment:\n",
    "        # Keep only alphanumerics and a few safe characters\n",
    "        fragment = \"\".join(c if c.isalnum() or c in \"._-\" else \"_\" for c in fragment)\n",
    "    \n",
    "    # Form the base filename from netloc, path, and fragment (if available)\n",
    "    base = f\"{netloc}_{path}\"\n",
    "    if fragment:\n",
    "        base = f\"{base}_{fragment}\"\n",
    "    \n",
    "    # Sanitize the base further: keep only alphanumerics, dots, underscores, or hyphens\n",
    "    safe_base = \"\".join(c if c.isalnum() or c in \"._-\" else \"_\" for c in base)\n",
    "    \n",
    "    # Compute a short hash of the full URL to ensure uniqueness\n",
    "    hash_digest = hashlib.md5(url.encode(\"utf-8\")).hexdigest()[:8]\n",
    "    \n",
    "    # Combine safe_base, hash, and extension\n",
    "    safe_filename = f\"{safe_base}_{hash_digest}{extension}\"\n",
    "    return safe_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_content(content: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove wrapping triple backticks (```) if present,\n",
    "    as well as any language hints (like \"json\") that might appear\n",
    "    immediately after the first set of backticks.\n",
    "    \n",
    "    Params:\n",
    "        - content: The content to normalize\n",
    "    \n",
    "    Returns:\n",
    "        - The normalized content\n",
    "    \"\"\"\n",
    "    stripped = content.strip()\n",
    "    # Remove any leading/trailing triple backticks\n",
    "    # This pattern removes the first line if it starts with ``` (optionally followed by a language)\n",
    "    # and the last line if it ends with ```\n",
    "    if stripped.startswith(\"```\") and stripped.endswith(\"```\"):\n",
    "        # Remove the first and last lines\n",
    "        lines = stripped.splitlines()\n",
    "        # If the first line is only backticks (or backticks with a language hint)\n",
    "        if lines[0].strip().startswith(\"```\"):\n",
    "            lines = lines[1:]\n",
    "        if lines and lines[-1].strip().endswith(\"```\"):\n",
    "            lines = lines[:-1]\n",
    "        normalized = \"\\n\".join(lines)\n",
    "        return normalized.strip()\n",
    "    return content\n",
    "\n",
    "def is_not_found_content(content: str) -> bool:\n",
    "    \"\"\"\n",
    "    Return True if content can be parsed as JSON and equals {\"detail\": \"Not Found\"}\n",
    "    (ignoring extra whitespace or differences in spacing).\n",
    "    \n",
    "    Params:\n",
    "        - content: The content to check\n",
    "    \n",
    "    Returns:\n",
    "        - True if the content matches the expected pattern, False otherwise\n",
    "    \"\"\"\n",
    "    normalized = normalize_content(content)\n",
    "    try:\n",
    "        data = json.loads(normalized)\n",
    "        # Check that it's a dict with a key \"detail\" whose value (case-insensitive) is \"not found\"\n",
    "        if isinstance(data, dict) and data.get(\"detail\", \"\").strip().lower() == \"not found\":\n",
    "            return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_href(href: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove angle brackets from the href and return a clean version.\n",
    "    \n",
    "    Params:\n",
    "        - href: the URL to sanitize\n",
    "    \n",
    "    Returns:\n",
    "        - the sanitized URL\n",
    "    \"\"\"\n",
    "    return href.strip(\"<>\")\n",
    "\n",
    "def resolve_relative_url(href: str, default_base: str) -> str:\n",
    "    \"\"\"\n",
    "    Resolve a relative href using a default base URL.\n",
    "    If href is already absolute, it is returned unchanged.\n",
    "    \n",
    "    Params:\n",
    "        - href:         the URL to resolve\n",
    "        - default_base: the default base URL to use if href is relative\n",
    "        \n",
    "    Returns:\n",
    "        - the resolved URL\n",
    "    \"\"\"\n",
    "    clean_href = sanitize_href(href)\n",
    "    if clean_href.startswith(\"http\"):\n",
    "        return clean_href\n",
    "    else:\n",
    "        return urljoin(default_base, clean_href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of URLs to crawl: ['https://stable-learn.com/en/lightrag-introduction/']\n"
     ]
    }
   ],
   "source": [
    "# --- Convert Widget Content to a URL List ---\n",
    "urls = url_input.value.strip().splitlines()\n",
    "print(\"List of URLs to crawl:\", urls)\n",
    "\n",
    "# --- Set Up Browser Configuration ---\n",
    "browser_cfg = BrowserConfig(\n",
    "    browser_type=\"chromium\",\n",
    "    headless=True,\n",
    "    verbose=True,\n",
    "    viewport_width=1280,\n",
    "    viewport_height=720,\n",
    ")\n",
    "\n",
    "# CrawlerRunConfig is set up to:\n",
    "# - Exclude external links (only internal links remain in result.links)\n",
    "run_cfg = CrawlerRunConfig(\n",
    "    cache_mode=CacheMode.BYPASS,\n",
    "    word_count_threshold=15,        # minimum word count for content\n",
    "    exclude_external_links=True,\n",
    "    stream=True,  # For efficient processing when using arun_many()/arun()\n",
    ")\n",
    "\n",
    "# --- Create output folder for crawled data ---\n",
    "output_folder = \"crawled_data\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# --- Define the Recursive Crawling Function ---\n",
    "async def crawl_recursive(\n",
    "    crawler: AsyncWebCrawler,\n",
    "    url: str,\n",
    "    visited: Set[str],\n",
    "    depth: int,\n",
    "    max_depth: int\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Recursively crawl a given URL using an AsyncWebCrawler, save its output, and follow internal links.\n",
    "\n",
    "    Params:\n",
    "        crawler (AsyncWebCrawler):  The crawler instance used to perform the web crawl.\n",
    "        url (str):                  The URL to crawl.\n",
    "        visited (Set[str]):         A set of URLs that have already been visited to avoid duplicate processing.\n",
    "        depth (int):                The current recursion depth.\n",
    "        max_depth (int):            The maximum allowed recursion depth.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"  \n",
    "    if depth > max_depth or url in visited:\n",
    "        return\n",
    "    visited.add(url)\n",
    "    \n",
    "    # Perform the crawl for a single URL\n",
    "    result = await crawler.arun(url, config=run_cfg)\n",
    "    if result.success:\n",
    "        print(f\"Crawled URL: {result.url} at depth {depth}\")\n",
    "        \n",
    "        # Attempt to use markdown output (preferably via markdown_v2 or markdown field)\n",
    "        md_content = None\n",
    "        if result.markdown:\n",
    "            if isinstance(result.markdown, str):\n",
    "                md_content = result.markdown\n",
    "            elif hasattr(result.markdown, \"raw_markdown\"):\n",
    "                md_content = result.markdown.raw_markdown\n",
    "        elif result.markdown_v2 and hasattr(result.markdown_v2, \"raw_markdown\"):\n",
    "            md_content = result.markdown_v2.raw_markdown\n",
    "        \n",
    "        if md_content:\n",
    "            if is_not_found_content(md_content):\n",
    "                print(f\"Skipping saving markdown for {url} because it indicates Not Found.\")\n",
    "            else:\n",
    "                filename = get_safe_filename(url, extension=\".md\")\n",
    "                file_path = os.path.join(output_folder, filename)\n",
    "                with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(md_content)\n",
    "                print(f\"Saved markdown to {file_path}\")\n",
    "        else:\n",
    "            # Fallback to cleaned HTML if no markdown is available.\n",
    "            content = result.cleaned_html or \"\"\n",
    "            if is_not_found_content(md_content):\n",
    "                print(f\"Skipping saving markdown for {url} because it indicates Not Found.\")\n",
    "            else:\n",
    "                filename = get_safe_filename(url, extension=\".html\")\n",
    "                file_path = os.path.join(output_folder, filename)\n",
    "                with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(content)\n",
    "                print(f\"Saved cleaned HTML to {file_path}\")\n",
    "\n",
    "        # Now, get the internal links from the CrawlResult.\n",
    "        # According to the API, result.links is a dictionary that may contain an \"internal\" key.\n",
    "        internal_links = result.links.get(\"internal\", [])\n",
    "        for link_info in internal_links:\n",
    "            href = link_info.get(\"href\")\n",
    "            if href:\n",
    "                # Convert relative URLs to absolute using the current URL as base\n",
    "                absolute_url = resolve_relative_url(href, url)\n",
    "                # Optionally, check that the domain is the same as the original URL\n",
    "                if urlparse(absolute_url).netloc == urlparse(url).netloc:\n",
    "                    await crawl_recursive(crawler, absolute_url, visited, depth + 1, max_depth)\n",
    "    else:\n",
    "        print(f\"Failed to crawl URL: {url}\")\n",
    "        print(f\"Error: {result.error_message}\")\n",
    "\n",
    "# --- Main Recursive Crawling Function ---\n",
    "async def crawl_main(start_urls: List[str], max_depth: int = 2) -> None:\n",
    "    \"\"\"\n",
    "    Main function to start recursive crawling for given start URLs.\n",
    "\n",
    "    Params:\n",
    "        start_urls (List[str]): List of URLs to begin crawling.\n",
    "        max_depth (int):        Maximum allowed recursion depth.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    visited = set()\n",
    "    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n",
    "        for url in start_urls:\n",
    "            await crawl_recursive(crawler, url, visited, depth=0, max_depth=max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INIT].... → Crawl4AI 0.4.248\n",
      "[FETCH]... ↓ https://stable-learn.com/en/lightrag-introduction/... | Status: True | Time: 3.30s\n",
      "[SCRAPE].. ◆ Processed https://stable-learn.com/en/lightrag-introduction/... | Time: 448ms\n",
      "[COMPLETE] ● https://stable-learn.com/en/lightrag-introduction/... | Status: True | Total: 3.77s\n",
      "Crawled URL: https://stable-learn.com/en/lightrag-introduction/ at depth 0\n",
      "Saved markdown to crawled_data\\stable-learn.com_en_lightrag-introduction_ad6535f6.md\n",
      "[FETCH]... ↓ https://stable-learn.com/en... | Status: True | Time: 3.33s\n",
      "[SCRAPE].. ◆ Processed https://stable-learn.com/en... | Time: 1114ms\n",
      "[COMPLETE] ● https://stable-learn.com/en... | Status: True | Total: 4.47s\n",
      "Crawled URL: https://stable-learn.com/en at depth 1\n",
      "Saved markdown to crawled_data\\stable-learn.com_en_e690ce76.md\n",
      "[FETCH]... ↓ https://stable-learn.com/en/tags... | Status: True | Time: 1.69s\n",
      "[SCRAPE].. ◆ Processed https://stable-learn.com/en/tags... | Time: 119ms\n",
      "[COMPLETE] ● https://stable-learn.com/en/tags... | Status: True | Total: 1.83s\n",
      "Crawled URL: https://stable-learn.com/en/tags at depth 1\n",
      "Saved markdown to crawled_data\\stable-learn.com_en_tags_2cd5d4f5.md\n",
      "[FETCH]... ↓ https://stable-learn.com/en/apps... | Status: True | Time: 2.01s\n",
      "[SCRAPE].. ◆ Processed https://stable-learn.com/en/apps... | Time: 927ms\n",
      "[COMPLETE] ● https://stable-learn.com/en/apps... | Status: True | Total: 2.96s\n",
      "Crawled URL: https://stable-learn.com/en/apps at depth 1\n",
      "Saved markdown to crawled_data\\stable-learn.com_en_apps_b0349917.md\n",
      "[FETCH]... ↓ https://stable-learn.com/en/tools... | Status: True | Time: 1.86s\n",
      "[SCRAPE].. ◆ Processed https://stable-learn.com/en/tools... | Time: 1088ms\n",
      "[COMPLETE] ● https://stable-learn.com/en/tools... | Status: True | Total: 2.97s\n",
      "Crawled URL: https://stable-learn.com/en/tools at depth 1\n",
      "Saved markdown to crawled_data\\stable-learn.com_en_tools_b1e3a1f1.md\n",
      "[FETCH]... ↓ https://stable-learn.com/en/tags/rag systems/... | Status: True | Time: 2.68s\n",
      "[SCRAPE].. ◆ Processed https://stable-learn.com/en/tags/rag systems/... | Time: 1101ms\n",
      "[COMPLETE] ● https://stable-learn.com/en/tags/rag systems/... | Status: True | Total: 3.81s\n",
      "Crawled URL: https://stable-learn.com/en/tags/rag systems/ at depth 1\n",
      "Saved markdown to crawled_data\\stable-learn.com_en_tags_rag_systems_81b45666.md\n",
      "[FETCH]... ↓ https://stable-learn.com/en/tags/lightrag/... | Status: True | Time: 2.38s\n",
      "[SCRAPE].. ◆ Processed https://stable-learn.com/en/tags/lightrag/... | Time: 1050ms\n",
      "[COMPLETE] ● https://stable-learn.com/en/tags/lightrag/... | Status: True | Total: 3.45s\n",
      "Crawled URL: https://stable-learn.com/en/tags/lightrag/ at depth 1\n",
      "Saved markdown to crawled_data\\stable-learn.com_en_tags_lightrag_41880ad5.md\n",
      "[FETCH]... ↓ https://stable-learn.com/en/tags/llm development/... | Status: True | Time: 2.57s\n",
      "[SCRAPE].. ◆ Processed https://stable-learn.com/en/tags/llm development/... | Time: 1531ms\n",
      "[COMPLETE] ● https://stable-learn.com/en/tags/llm development/... | Status: True | Total: 4.12s\n",
      "Crawled URL: https://stable-learn.com/en/tags/llm development/ at depth 1\n",
      "Saved markdown to crawled_data\\stable-learn.com_en_tags_llm_development_65abd505.md\n",
      "[FETCH]... ↓ https://stable-learn.com/en/tags/knowledge graphs/... | Status: True | Time: 2.94s\n",
      "[SCRAPE].. ◆ Processed https://stable-learn.com/en/tags/knowledge graphs/... | Time: 892ms\n",
      "[COMPLETE] ● https://stable-learn.com/en/tags/knowledge graphs/... | Status: True | Total: 3.86s\n",
      "Crawled URL: https://stable-learn.com/en/tags/knowledge graphs/ at depth 1\n",
      "Saved markdown to crawled_data\\stable-learn.com_en_tags_knowledge_graphs_d0cef8b5.md\n",
      "[FETCH]... ↓ https://stable-learn.com/en/tags/smart qa/... | Status: True | Time: 2.20s\n",
      "[SCRAPE].. ◆ Processed https://stable-learn.com/en/tags/smart qa/... | Time: 905ms\n",
      "[COMPLETE] ● https://stable-learn.com/en/tags/smart qa/... | Status: True | Total: 3.13s\n",
      "Crawled URL: https://stable-learn.com/en/tags/smart qa/ at depth 1\n",
      "Saved markdown to crawled_data\\stable-learn.com_en_tags_smart_qa_d1371b43.md\n",
      "[FETCH]... ↓ https://stable-learn.com/en/tags/ai tutorial/... | Status: True | Time: 2.18s\n",
      "[SCRAPE].. ◆ Processed https://stable-learn.com/en/tags/ai tutorial/... | Time: 199ms\n",
      "[COMPLETE] ● https://stable-learn.com/en/tags/ai tutorial/... | Status: True | Total: 2.40s\n",
      "Crawled URL: https://stable-learn.com/en/tags/ai tutorial/ at depth 1\n",
      "Saved markdown to crawled_data\\stable-learn.com_en_tags_ai_tutorial_046d1950.md\n",
      "[FETCH]... ↓ https://stable-learn.com/en/tags/llm applications/... | Status: True | Time: 2.11s\n",
      "[SCRAPE].. ◆ Processed https://stable-learn.com/en/tags/llm applications/... | Time: 1110ms\n",
      "[COMPLETE] ● https://stable-learn.com/en/tags/llm applications/... | Status: True | Total: 3.24s\n",
      "Crawled URL: https://stable-learn.com/en/tags/llm applications/ at depth 1\n",
      "Saved markdown to crawled_data\\stable-learn.com_en_tags_llm_applications_241c85da.md\n",
      "[FETCH]... ↓ https://stable-learn.com/en/microsoft-omniparser-v... | Status: True | Time: 3.02s\n",
      "[SCRAPE].. ◆ Processed https://stable-learn.com/en/microsoft-omniparser-v... | Time: 868ms\n",
      "[COMPLETE] ● https://stable-learn.com/en/microsoft-omniparser-v... | Status: True | Total: 3.91s\n",
      "Crawled URL: https://stable-learn.com/en/microsoft-omniparser-v2-release/ at depth 1\n",
      "Saved markdown to crawled_data\\stable-learn.com_en_microsoft-omniparser-v2-release_91795615.md\n",
      "[FETCH]... ↓ https://stable-learn.com/en/goku-video-model-intro... | Status: True | Time: 3.39s\n",
      "[SCRAPE].. ◆ Processed https://stable-learn.com/en/goku-video-model-intro... | Time: 1341ms\n",
      "[COMPLETE] ● https://stable-learn.com/en/goku-video-model-intro... | Status: True | Total: 4.75s\n",
      "Crawled URL: https://stable-learn.com/en/goku-video-model-introduction/ at depth 1\n",
      "Saved markdown to crawled_data\\stable-learn.com_en_goku-video-model-introduction_ea5b4131.md\n",
      "[FETCH]... ↓ https://stable-learn.com/en/llm-reasoner-tutorial/... | Status: True | Time: 2.92s\n",
      "[SCRAPE].. ◆ Processed https://stable-learn.com/en/llm-reasoner-tutorial/... | Time: 1143ms\n",
      "[COMPLETE] ● https://stable-learn.com/en/llm-reasoner-tutorial/... | Status: True | Total: 4.09s\n",
      "Crawled URL: https://stable-learn.com/en/llm-reasoner-tutorial/ at depth 1\n",
      "Saved markdown to crawled_data\\stable-learn.com_en_llm-reasoner-tutorial_2c335f22.md\n",
      "[FETCH]... ↓ https://stable-learn.com/en/deepseek-r1-paper-arti... | Status: True | Time: 2.80s\n",
      "[SCRAPE].. ◆ Processed https://stable-learn.com/en/deepseek-r1-paper-arti... | Time: 1008ms\n",
      "[COMPLETE] ● https://stable-learn.com/en/deepseek-r1-paper-arti... | Status: True | Total: 3.83s\n",
      "Crawled URL: https://stable-learn.com/en/deepseek-r1-paper-article/ at depth 1\n",
      "Saved markdown to crawled_data\\stable-learn.com_en_deepseek-r1-paper-article_dc0156b1.md\n",
      "[FETCH]... ↓ https://stable-learn.com/en/lets-encrypt-6-day-cer... | Status: True | Time: 2.81s\n",
      "[SCRAPE].. ◆ Processed https://stable-learn.com/en/lets-encrypt-6-day-cer... | Time: 841ms\n",
      "[COMPLETE] ● https://stable-learn.com/en/lets-encrypt-6-day-cer... | Status: True | Total: 3.67s\n",
      "Crawled URL: https://stable-learn.com/en/lets-encrypt-6-day-certificates-2025/ at depth 1\n",
      "Saved markdown to crawled_data\\stable-learn.com_en_lets-encrypt-6-day-certificates-2025_459768ff.md\n",
      "[FETCH]... ↓ https://stable-learn.com/en/2024-december-openai-1... | Status: True | Time: 4.62s\n",
      "[SCRAPE].. ◆ Processed https://stable-learn.com/en/2024-december-openai-1... | Time: 1143ms\n",
      "[COMPLETE] ● https://stable-learn.com/en/2024-december-openai-1... | Status: True | Total: 5.79s\n",
      "Crawled URL: https://stable-learn.com/en/2024-december-openai-12-day-technical-live-highlights-detailed-report/ at depth 1\n",
      "Saved markdown to crawled_data\\stable-learn.com_en_2024-december-openai-12-day-technical-live-highlights-detailed-report_171b5322.md\n",
      "[FETCH]... ↓ https://stable-learn.com/en/ai-model-tools-compari... | Status: True | Time: 3.00s\n",
      "[SCRAPE].. ◆ Processed https://stable-learn.com/en/ai-model-tools-compari... | Time: 905ms\n",
      "[COMPLETE] ● https://stable-learn.com/en/ai-model-tools-compari... | Status: True | Total: 3.93s\n",
      "Crawled URL: https://stable-learn.com/en/ai-model-tools-comparison/ at depth 1\n",
      "Saved markdown to crawled_data\\stable-learn.com_en_ai-model-tools-comparison_172ca829.md\n",
      "[FETCH]... ↓ https://stable-learn.com/en/ant-design-x-introduct... | Status: True | Time: 2.83s\n",
      "[SCRAPE].. ◆ Processed https://stable-learn.com/en/ant-design-x-introduct... | Time: 1011ms\n",
      "[COMPLETE] ● https://stable-learn.com/en/ant-design-x-introduct... | Status: True | Total: 3.86s\n",
      "Crawled URL: https://stable-learn.com/en/ant-design-x-introduction/ at depth 1\n",
      "Saved markdown to crawled_data\\stable-learn.com_en_ant-design-x-introduction_f264c6ce.md\n",
      "[FETCH]... ↓ https://stable-learn.com/en/ces-2024-review/... | Status: True | Time: 11.55s\n",
      "[SCRAPE].. ◆ Processed https://stable-learn.com/en/ces-2024-review/... | Time: 1366ms\n",
      "[COMPLETE] ● https://stable-learn.com/en/ces-2024-review/... | Status: True | Total: 12.95s\n",
      "Crawled URL: https://stable-learn.com/en/ces-2024-review/ at depth 1\n",
      "Saved markdown to crawled_data\\stable-learn.com_en_ces-2024-review_7ef6a7ae.md\n",
      "[FETCH]... ↓ https://stable-learn.com/en/ces-2025-vlc-ai-offlin... | Status: True | Time: 3.82s\n",
      "[SCRAPE].. ◆ Processed https://stable-learn.com/en/ces-2025-vlc-ai-offlin... | Time: 1069ms\n",
      "[COMPLETE] ● https://stable-learn.com/en/ces-2025-vlc-ai-offlin... | Status: True | Total: 4.91s\n",
      "Crawled URL: https://stable-learn.com/en/ces-2025-vlc-ai-offline/ at depth 1\n",
      "Saved markdown to crawled_data\\stable-learn.com_en_ces-2025-vlc-ai-offline_1b151398.md\n",
      "[FETCH]... ↓ https://stable-learn.com/en/clearvoice-studio-tuto... | Status: True | Time: 3.54s\n",
      "[SCRAPE].. ◆ Processed https://stable-learn.com/en/clearvoice-studio-tuto... | Time: 1373ms\n",
      "[COMPLETE] ● https://stable-learn.com/en/clearvoice-studio-tuto... | Status: True | Total: 4.94s\n",
      "Crawled URL: https://stable-learn.com/en/clearvoice-studio-tutorial/ at depth 1\n",
      "Saved markdown to crawled_data\\stable-learn.com_en_clearvoice-studio-tutorial_60419ba6.md\n",
      "[FETCH]... ↓ https://stable-learn.com/en/cogagent-introduction/... | Status: True | Time: 2.99s\n",
      "[SCRAPE].. ◆ Processed https://stable-learn.com/en/cogagent-introduction/... | Time: 1345ms\n",
      "[COMPLETE] ● https://stable-learn.com/en/cogagent-introduction/... | Status: True | Total: 4.37s\n",
      "Crawled URL: https://stable-learn.com/en/cogagent-introduction/ at depth 1\n",
      "Saved markdown to crawled_data\\stable-learn.com_en_cogagent-introduction_fa6d2b9f.md\n",
      "[FETCH]... ↓ https://stable-learn.com/en/comfyui_install_window... | Status: True | Time: 2.89s\n",
      "[SCRAPE].. ◆ Processed https://stable-learn.com/en/comfyui_install_window... | Time: 1122ms\n",
      "[COMPLETE] ● https://stable-learn.com/en/comfyui_install_window... | Status: True | Total: 4.03s\n",
      "Crawled URL: https://stable-learn.com/en/comfyui_install_windows/ at depth 1\n",
      "Saved markdown to crawled_data\\stable-learn.com_en_comfyui_install_windows_08061899.md\n",
      "[FETCH]... ↓ https://stable-learn.com/en/deepdeek-v3-ai-about/... | Status: True | Time: 2.96s\n",
      "[SCRAPE].. ◆ Processed https://stable-learn.com/en/deepdeek-v3-ai-about/... | Time: 1098ms\n",
      "[COMPLETE] ● https://stable-learn.com/en/deepdeek-v3-ai-about/... | Status: True | Total: 4.08s\n",
      "Crawled URL: https://stable-learn.com/en/deepdeek-v3-ai-about/ at depth 1\n",
      "Saved markdown to crawled_data\\stable-learn.com_en_deepdeek-v3-ai-about_24a32880.md\n",
      "[FETCH]... ↓ https://stable-learn.com/en/category/news/1... | Status: True | Time: 3.00s\n",
      "[SCRAPE].. ◆ Processed https://stable-learn.com/en/category/news/1... | Time: 949ms\n",
      "[COMPLETE] ● https://stable-learn.com/en/category/news/1... | Status: True | Total: 3.97s\n",
      "Crawled URL: https://stable-learn.com/en/category/news/1 at depth 1\n",
      "Saved markdown to crawled_data\\stable-learn.com_en_category_news_1_469f3afb.md\n",
      "[FETCH]... ↓ https://stable-learn.com/en/eino-open-source-annou... | Status: True | Time: 3.76s\n",
      "[SCRAPE].. ◆ Processed https://stable-learn.com/en/eino-open-source-annou... | Time: 1316ms\n",
      "[COMPLETE] ● https://stable-learn.com/en/eino-open-source-annou... | Status: True | Total: 5.10s\n",
      "Crawled URL: https://stable-learn.com/en/eino-open-source-announcement/ at depth 1\n",
      "Saved markdown to crawled_data\\stable-learn.com_en_eino-open-source-announcement_2bff9521.md\n",
      "[FETCH]... ↓ https://stable-learn.com/en/nvidia-rtc-5090-pub/... | Status: True | Time: 4.81s\n",
      "[SCRAPE].. ◆ Processed https://stable-learn.com/en/nvidia-rtc-5090-pub/... | Time: 870ms\n",
      "[COMPLETE] ● https://stable-learn.com/en/nvidia-rtc-5090-pub/... | Status: True | Total: 5.70s\n",
      "Crawled URL: https://stable-learn.com/en/nvidia-rtc-5090-pub/ at depth 1\n",
      "Saved markdown to crawled_data\\stable-learn.com_en_nvidia-rtc-5090-pub_4292153f.md\n",
      "[FETCH]... ↓ https://stable-learn.com/en/one-api-docker-securit... | Status: True | Time: 3.48s\n",
      "[SCRAPE].. ◆ Processed https://stable-learn.com/en/one-api-docker-securit... | Time: 1491ms\n",
      "[COMPLETE] ● https://stable-learn.com/en/one-api-docker-securit... | Status: True | Total: 4.99s\n",
      "Crawled URL: https://stable-learn.com/en/one-api-docker-security-incident/ at depth 1\n",
      "Saved markdown to crawled_data\\stable-learn.com_en_one-api-docker-security-incident_1c9233ca.md\n"
     ]
    }
   ],
   "source": [
    "def run_crawl_in_thread() -> None:\n",
    "    \"\"\"Run the recursive crawl in a separate thread.\"\"\"\n",
    "    loop = asyncio.new_event_loop()\n",
    "    asyncio.set_event_loop(loop)\n",
    "    loop.run_until_complete(crawl_main(urls, max_depth=1))\n",
    "    loop.close()\n",
    "\n",
    "t = threading.Thread(target=run_crawl_in_thread)\n",
    "t.start()\n",
    "t.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LightRAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import glob\n",
    "import numpy as np\n",
    "from lightrag import LightRAG, QueryParam\n",
    "from lightrag.llm.ollama import ollama_model_complete, ollama_embedding\n",
    "from lightrag.utils import EmbeddingFunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a wrapper for the embedding function that converts output to np.float32\n",
    "async def my_embedding_func(texts: list[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Async wrapper for ollama_embedding that returns embeddings as a np.float32 array.\n",
    "    \n",
    "    Params:\n",
    "        texts (List[str]): List of input texts to embed.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Embeddings as a float32 NumPy array.\n",
    "    \"\"\"\n",
    "    embeddings = await ollama_embedding(\n",
    "        texts, \n",
    "        embed_model=\"nomic-embed-text\", \n",
    "        host=\"http://localhost:11434\"\n",
    "    )\n",
    "    # Ensure the embeddings are in a NumPy array with dtype float32\n",
    "    return np.array(embeddings, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Logger initialized for working directory: ./lightRAG_db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Load KV json_doc_status_storage with 0 data\n",
      "INFO:Load KV llm_response_cache with 1 data\n",
      "INFO:Load KV full_docs with 8 data\n",
      "INFO:Load KV text_chunks with 33 data\n",
      "INFO:Loaded graph from ./lightRAG_db\\graph_chunk_entity_relation.graphml with 66 nodes, 12 edges\n",
      "INFO:Loading faiss with AVX2 support.\n",
      "INFO:Successfully loaded faiss with AVX2 support.\n",
      "INFO:Faiss index loaded with 63 vectors from ./lightRAG_db\\faiss_index_entities.index\n",
      "INFO:Faiss index loaded with 12 vectors from ./lightRAG_db\\faiss_index_relationships.index\n",
      "INFO:Faiss index loaded with 33 vectors from ./lightRAG_db\\faiss_index_chunks.index\n",
      "INFO:Loaded document status storage with 9 records\n"
     ]
    }
   ],
   "source": [
    "# Set logging level to see info messages\n",
    "logging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.INFO)\n",
    "\n",
    "# Create a working directory where LightRAG will store its cache and index data\n",
    "WORKING_DIR = \"./lightRAG_db\"\n",
    "os.makedirs(WORKING_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize LightRAG with the Ollama model for completion and embedding,\n",
    "# and use FAISS as the vector storage backend.\n",
    "rag = LightRAG(\n",
    "    working_dir=WORKING_DIR,\n",
    "        addon_params={\n",
    "        \"insert_batch_size\": 1             # Process 2 documents per batch\n",
    "    },\n",
    "    llm_model_func=ollama_model_complete,  # Function for generating completions via Ollama \n",
    "    llm_model_name=\"qwen2.5:1.5b\",       # Specify the Ollama model to use (Gemma 2B in this example) \n",
    "    llm_model_max_async=1,                 # Maximum concurrent requests to the LLM service\n",
    "    llm_model_max_token_size=32768,        # LLM must support at least 32k tokens of context\n",
    "    llm_model_kwargs={\n",
    "        \"host\": \"http://localhost:11434\",  # Ollama service address\n",
    "        \"options\": {\"num_ctx\": 32768},     # Additional options; set context window size\n",
    "        # resoning_tag: \"think\",           # For DeepSeek models, set reasoning_tag to \"think\"\n",
    "    },\n",
    "    embedding_func=EmbeddingFunc(\n",
    "        embedding_dim=768,                # Dimension of the embedding vectors\n",
    "        max_token_size=8192,              # Maximum token size for embedding generation\n",
    "        func=my_embedding_func,           # Function to generate embeddings\n",
    "    ),\n",
    "    vector_storage=\"FaissVectorDBStorage\",  # Use FAISS for vector storage (use faiss-gpu if available)\n",
    "    vector_db_storage_cls_kwargs={\n",
    "        \"cosine_better_than_threshold\": 0.3  # Threshold for retrieval similarity; adjust as needed    \n",
    "    },\n",
    "    chunk_token_size=1000,\n",
    "    chunk_overlap_token_size=100,   \n",
    ")\n",
    "# interesting models for my use case (gpu with 3gb ram): \n",
    "# qwen2.5:1.5b, granite3.1-moe:1b\n",
    "\n",
    "# Rule of thumb: double parameter size of model to get gpu memory requirement: 1.5b -> 3gb, 7b -> 14gb\n",
    "# Dont forget that you need to have head space for the context that goes into the model \n",
    "\n",
    "# deepseek-r1:1.5b - To return only the model's response, you can pass reasoning_tag in llm_model_kwargs.\n",
    "# For example, for DeepSeek models, reasoning_tag should be set to think  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Processing 35 new unique documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 71 documents from crawled_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch 1:   0%|          | 0/1 [00:00<?, ?it/s]INFO:Inserting 3 vectors to chunks\n",
      "INFO:Upserted 3 vectors into Faiss index.\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.89s/batch]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[AINFO:Inserting 12 vectors to entities\n",
      "INFO:Upserted 12 vectors into Faiss index.\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:14<00:00, 14.60s/batch]\n",
      "INFO:Inserting 5 vectors to relationships\n",
      "INFO:Upserted 5 vectors into Faiss index.\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.05s/batch]\n",
      "INFO:Faiss index saved successfully.\n",
      "INFO:Faiss index saved successfully.\n",
      "INFO:Faiss index saved successfully.\n",
      "INFO:Writing graph with 12 nodes, 5 edges\n",
      "Processing batch 1: 100%|██████████| 1/1 [29:36<00:00, 1776.34s/it]\n",
      "Processing batch 2:   0%|          | 0/1 [00:00<?, ?it/s]INFO:Inserting 2 vectors to chunks\n",
      "INFO:Upserted 2 vectors into Faiss index.\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.32s/batch]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[AWARNING:Didn't extract any relationships\n",
      "INFO:Inserting 2 vectors to entities\n",
      "INFO:Upserted 2 vectors into Faiss index.\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.55s/batch]\n",
      "INFO:Inserting 0 vectors to relationships\n",
      "WARNING:You are inserting empty data to the vector DB\n",
      "INFO:Faiss index saved successfully.\n",
      "INFO:Faiss index saved successfully.\n",
      "INFO:Faiss index saved successfully.\n",
      "INFO:Writing graph with 13 nodes, 5 edges\n",
      "Processing batch 2: 100%|██████████| 1/1 [17:38<00:00, 1058.84s/it]\n",
      "Processing batch 3:   0%|          | 0/1 [00:00<?, ?it/s]INFO:Inserting 4 vectors to chunks\n",
      "INFO:Upserted 4 vectors into Faiss index.\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.65s/batch]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[AWARNING:Didn't extract any relationships\n",
      "INFO:Inserting 10 vectors to entities\n",
      "INFO:Upserted 10 vectors into Faiss index.\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.35s/batch]\n",
      "INFO:Inserting 0 vectors to relationships\n",
      "WARNING:You are inserting empty data to the vector DB\n",
      "INFO:Faiss index saved successfully.\n",
      "INFO:Faiss index saved successfully.\n",
      "INFO:Faiss index saved successfully.\n",
      "INFO:Writing graph with 22 nodes, 5 edges\n",
      "Processing batch 3: 100%|██████████| 1/1 [46:24<00:00, 2784.23s/it]\n",
      "Processing batch 4:   0%|          | 0/1 [00:00<?, ?it/s]INFO:Inserting 3 vectors to chunks\n",
      "INFO:Upserted 3 vectors into Faiss index.\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.40s/batch]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[AINFO:Inserting 21 vectors to entities\n",
      "INFO:Upserted 21 vectors into Faiss index.\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.72s/batch]\n",
      "INFO:Inserting 1 vectors to relationships\n",
      "INFO:Upserted 1 vectors into Faiss index.\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:02<00:00,  2.80s/batch]\n",
      "INFO:Faiss index saved successfully.\n",
      "INFO:Faiss index saved successfully.\n",
      "INFO:Faiss index saved successfully.\n",
      "INFO:Writing graph with 39 nodes, 6 edges\n",
      "Processing batch 4: 100%|██████████| 1/1 [27:18<00:00, 1638.42s/it]\n",
      "Processing batch 5:   0%|          | 0/1 [00:00<?, ?it/s]INFO:Inserting 4 vectors to chunks\n",
      "INFO:Upserted 4 vectors into Faiss index.\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.70s/batch]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[AINFO:Inserting 4 vectors to entities\n",
      "INFO:Upserted 4 vectors into Faiss index.\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.24s/batch]\n",
      "INFO:Inserting 5 vectors to relationships\n",
      "INFO:Upserted 5 vectors into Faiss index.\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.01s/batch]\n",
      "INFO:Faiss index saved successfully.\n",
      "INFO:Faiss index saved successfully.\n",
      "INFO:Faiss index saved successfully.\n",
      "INFO:Writing graph with 39 nodes, 6 edges\n",
      "Processing batch 5: 100%|██████████| 1/1 [38:47<00:00, 2327.55s/it]\n",
      "Processing batch 6:   0%|          | 0/1 [00:00<?, ?it/s]INFO:Inserting 4 vectors to chunks\n",
      "INFO:Upserted 4 vectors into Faiss index.\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.57s/batch]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[AWARNING:Didn't extract any relationships\n",
      "INFO:Inserting 23 vectors to entities\n",
      "INFO:Upserted 23 vectors into Faiss index.\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.62s/batch]\n",
      "INFO:Inserting 0 vectors to relationships\n",
      "WARNING:You are inserting empty data to the vector DB\n",
      "INFO:Faiss index saved successfully.\n",
      "INFO:Faiss index saved successfully.\n",
      "INFO:Faiss index saved successfully.\n",
      "INFO:Writing graph with 48 nodes, 6 edges\n",
      "Processing batch 6: 100%|██████████| 1/1 [35:11<00:00, 2111.79s/it]\n",
      "Processing batch 7:   0%|          | 0/1 [00:00<?, ?it/s]INFO:Inserting 6 vectors to chunks\n",
      "INFO:Upserted 6 vectors into Faiss index.\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.79s/batch]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[AINFO:Inserting 15 vectors to entities\n",
      "INFO:Upserted 15 vectors into Faiss index.\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.74s/batch]\n",
      "INFO:Inserting 5 vectors to relationships\n",
      "INFO:Upserted 5 vectors into Faiss index.\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:02<00:00,  2.96s/batch]\n",
      "INFO:Faiss index saved successfully.\n",
      "INFO:Faiss index saved successfully.\n",
      "INFO:Faiss index saved successfully.\n",
      "INFO:Writing graph with 58 nodes, 11 edges\n",
      "Processing batch 7: 100%|██████████| 1/1 [57:24<00:00, 3444.62s/it]\n",
      "Processing batch 8:   0%|          | 0/1 [00:00<?, ?it/s]INFO:Inserting 7 vectors to chunks\n",
      "INFO:Upserted 7 vectors into Faiss index.\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.32s/batch]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[AINFO:Inserting 13 vectors to entities\n",
      "INFO:Upserted 13 vectors into Faiss index.\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.56s/batch]\n",
      "INFO:Inserting 1 vectors to relationships\n",
      "INFO:Upserted 1 vectors into Faiss index.\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:02<00:00,  2.84s/batch]\n",
      "INFO:Faiss index saved successfully.\n",
      "INFO:Faiss index saved successfully.\n",
      "INFO:Faiss index saved successfully.\n",
      "INFO:Writing graph with 66 nodes, 12 edges\n",
      "Processing batch 8: 100%|██████████| 1/1 [1:08:28<00:00, 4108.37s/it]\n",
      "Processing batch 9:   0%|          | 0/1 [00:00<?, ?it/s]INFO:Inserting 3 vectors to chunks\n",
      "INFO:Upserted 3 vectors into Faiss index.\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:03<00:00,  3.51s/batch]\n"
     ]
    }
   ],
   "source": [
    "# --- Load Crawled Documents ---\n",
    "data_folder = \"crawled_data\"\n",
    "md_files = glob.glob(os.path.join(data_folder, \"*.md\"))\n",
    "documents = []\n",
    "for file in md_files:\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read().strip()\n",
    "        # Optionally, skip documents that are empty\n",
    "        if text:\n",
    "            documents.append(text)\n",
    "print(f\"Loaded {len(documents)} documents from {data_folder}\")\n",
    "\n",
    "# --- Insert the Crawled Documents into LightRAG Asynchronously ---\n",
    "await rag.ainsert(documents)\n",
    "print(\"Documents inserted into LightRAG.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Querying the LightRAG System ---\n",
    "# Define a query and test different retrieval modes: \"naive\", \"local\", \"global\", and \"hybrid\".\n",
    "query = \"Please explain why Crawl4AI uses markdown for its output.\"\n",
    "modes = [\"naive\", \"local\", \"global\", \"hybrid\"]\n",
    "\n",
    "for mode in modes:\n",
    "    print(f\"\\nResults using {mode} mode:\")\n",
    "    result = rag.query(query, param=QueryParam(mode=mode))\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Website Crawling and Data Extraction\n",
    "\n",
    "In this section, we use `crawl4ai` to crawl one or more websites. The extracted text is saved as `.md` files in a folder (e.g., `./crawled_data`).\n",
    "\n",
    "You can enter the URLs interactively using the provided widget.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you run into issues with the event loop in Jupyter Notebook, uncomment the next two lines:\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Imports used for Crawl4AI part\n",
    "import asyncio\n",
    "import threading\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from typing import Set, Any, List\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "from urllib.parse import urlparse\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "# Import Crawl4AI classes based on the documentation\n",
    "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set Windows Proactor Event Loop (for Windows users) ---\n",
    "import sys\n",
    "if sys.platform == \"win32\":\n",
    "    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the following cell and the use the widget to set the urls for the crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- URL Input Widget ---\n",
    "url_input = widgets.Textarea(\n",
    "    value='https://crawl4ai.com/mkdocs/\\nhttps://stable-learn.com/en/lightrag-introduction/',\n",
    "    placeholder='Enter one URL per line',\n",
    "    description='URLs:',\n",
    "    layout=widgets.Layout(width='100%', height='80px')\n",
    ")\n",
    "display(url_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_safe_filename(url: str, extension: str = \".md\") -> str:\n",
    "    \"\"\"\n",
    "    Generate a safe and unique filename from a URL.\n",
    "    \n",
    "    The filename is based on the domain, path, and (if present) fragment of the URL.\n",
    "    A short hash of the full URL is appended to ensure uniqueness.\n",
    "    \n",
    "    Examples:\n",
    "      - \"https://crawl4ai.com/\" becomes something like \"crawl4ai.com_index_<hash>.md\"\n",
    "      - \"https://crawl4ai.com/#quick-start\" becomes \"crawl4ai.com_index_quick_start_<hash>.md\"\n",
    "      \n",
    "      \n",
    "    Params:\n",
    "        url: The URL to generate a filename for.\n",
    "        extension: The file extension to use (default: \".md\").\n",
    "        \n",
    "    Returns:\n",
    "        A safe and unique filename based on the URL.\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    \n",
    "    # Get the netloc and sanitize it (replace ':' with '_' if needed)\n",
    "    netloc = parsed.netloc.replace(\":\", \"_\")\n",
    "    \n",
    "    # Use the path; default to \"index\" if empty\n",
    "    path = parsed.path.strip(\"/\")\n",
    "    if not path:\n",
    "        path = \"index\"\n",
    "    \n",
    "    # Use the fragment if present (replace any non-alphanumeric characters with underscores)\n",
    "    fragment = parsed.fragment.strip()\n",
    "    if fragment:\n",
    "        # Keep only alphanumerics and a few safe characters\n",
    "        fragment = \"\".join(c if c.isalnum() or c in \"._-\" else \"_\" for c in fragment)\n",
    "    \n",
    "    # Form the base filename from netloc, path, and fragment (if available)\n",
    "    base = f\"{netloc}_{path}\"\n",
    "    if fragment:\n",
    "        base = f\"{base}_{fragment}\"\n",
    "    \n",
    "    # Sanitize the base further: keep only alphanumerics, dots, underscores, or hyphens\n",
    "    safe_base = \"\".join(c if c.isalnum() or c in \"._-\" else \"_\" for c in base)\n",
    "    \n",
    "    # Compute a short hash of the full URL to ensure uniqueness\n",
    "    hash_digest = hashlib.md5(url.encode(\"utf-8\")).hexdigest()[:8]\n",
    "    \n",
    "    # Combine safe_base, hash, and extension\n",
    "    safe_filename = f\"{safe_base}_{hash_digest}{extension}\"\n",
    "    return safe_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_content(content: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove wrapping triple backticks (```) if present,\n",
    "    as well as any language hints (like \"json\") that might appear\n",
    "    immediately after the first set of backticks.\n",
    "    \n",
    "    Params:\n",
    "        - content: The content to normalize\n",
    "    \n",
    "    Returns:\n",
    "        - The normalized content\n",
    "    \"\"\"\n",
    "    stripped = content.strip()\n",
    "    # Remove any leading/trailing triple backticks\n",
    "    # This pattern removes the first line if it starts with ``` (optionally followed by a language)\n",
    "    # and the last line if it ends with ```\n",
    "    if stripped.startswith(\"```\") and stripped.endswith(\"```\"):\n",
    "        # Remove the first and last lines\n",
    "        lines = stripped.splitlines()\n",
    "        # If the first line is only backticks (or backticks with a language hint)\n",
    "        if lines[0].strip().startswith(\"```\"):\n",
    "            lines = lines[1:]\n",
    "        if lines and lines[-1].strip().endswith(\"```\"):\n",
    "            lines = lines[:-1]\n",
    "        normalized = \"\\n\".join(lines)\n",
    "        return normalized.strip()\n",
    "    return content\n",
    "\n",
    "def is_not_found_content(content: str) -> bool:\n",
    "    \"\"\"\n",
    "    Return True if content can be parsed as JSON and equals {\"detail\": \"Not Found\"}\n",
    "    (ignoring extra whitespace or differences in spacing).\n",
    "    \n",
    "    Params:\n",
    "        - content: The content to check\n",
    "    \n",
    "    Returns:\n",
    "        - True if the content matches the expected pattern, False otherwise\n",
    "    \"\"\"\n",
    "    normalized = normalize_content(content)\n",
    "    try:\n",
    "        data = json.loads(normalized)\n",
    "        # Check that it's a dict with a key \"detail\" whose value (case-insensitive) is \"not found\"\n",
    "        if isinstance(data, dict) and data.get(\"detail\", \"\").strip().lower() == \"not found\":\n",
    "            return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_href(href: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove angle brackets from the href and return a clean version.\n",
    "    \n",
    "    Params:\n",
    "        - href: the URL to sanitize\n",
    "    \n",
    "    Returns:\n",
    "        - the sanitized URL\n",
    "    \"\"\"\n",
    "    return href.strip(\"<>\")\n",
    "\n",
    "def resolve_relative_url(href: str, default_base: str) -> str:\n",
    "    \"\"\"\n",
    "    Resolve a relative href using a default base URL.\n",
    "    If href is already absolute, it is returned unchanged.\n",
    "    \n",
    "    Params:\n",
    "        - href:         the URL to resolve\n",
    "        - default_base: the default base URL to use if href is relative\n",
    "        \n",
    "    Returns:\n",
    "        - the resolved URL\n",
    "    \"\"\"\n",
    "    clean_href = sanitize_href(href)\n",
    "    if clean_href.startswith(\"http\"):\n",
    "        return clean_href\n",
    "    else:\n",
    "        return urljoin(default_base, clean_href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Convert Widget Content to a URL List ---\n",
    "urls = url_input.value.strip().splitlines()\n",
    "print(\"List of URLs to crawl:\", urls)\n",
    "\n",
    "# --- Set Up Browser Configuration ---\n",
    "browser_cfg = BrowserConfig(\n",
    "    browser_type=\"chromium\",\n",
    "    headless=True,\n",
    "    verbose=True,\n",
    "    viewport_width=1280,\n",
    "    viewport_height=720,\n",
    ")\n",
    "\n",
    "# CrawlerRunConfig is set up to:\n",
    "# - Exclude external links (only internal links remain in result.links)\n",
    "run_cfg = CrawlerRunConfig(\n",
    "    cache_mode=CacheMode.BYPASS,\n",
    "    word_count_threshold=15,        # minimum word count for content\n",
    "    exclude_external_links=True,\n",
    "    stream=True,  # For efficient processing when using arun_many()/arun()\n",
    ")\n",
    "\n",
    "# --- Create output folder for crawled data ---\n",
    "output_folder = \"crawled_data\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# --- Define the Recursive Crawling Function ---\n",
    "async def crawl_recursive(\n",
    "    crawler: AsyncWebCrawler,\n",
    "    url: str,\n",
    "    visited: Set[str],\n",
    "    depth: int,\n",
    "    max_depth: int\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Recursively crawl a given URL using an AsyncWebCrawler, save its output, and follow internal links.\n",
    "\n",
    "    Params:\n",
    "        crawler (AsyncWebCrawler):  The crawler instance used to perform the web crawl.\n",
    "        url (str):                  The URL to crawl.\n",
    "        visited (Set[str]):         A set of URLs that have already been visited to avoid duplicate processing.\n",
    "        depth (int):                The current recursion depth.\n",
    "        max_depth (int):            The maximum allowed recursion depth.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"  \n",
    "    if depth > max_depth or url in visited:\n",
    "        return\n",
    "    visited.add(url)\n",
    "    \n",
    "    # Perform the crawl for a single URL\n",
    "    result = await crawler.arun(url, config=run_cfg)\n",
    "    if result.success:\n",
    "        print(f\"Crawled URL: {result.url} at depth {depth}\")\n",
    "        \n",
    "        # Attempt to use markdown output (preferably via markdown_v2 or markdown field)\n",
    "        md_content = None\n",
    "        if result.markdown:\n",
    "            if isinstance(result.markdown, str):\n",
    "                md_content = result.markdown\n",
    "            elif hasattr(result.markdown, \"raw_markdown\"):\n",
    "                md_content = result.markdown.raw_markdown\n",
    "        elif result.markdown_v2 and hasattr(result.markdown_v2, \"raw_markdown\"):\n",
    "            md_content = result.markdown_v2.raw_markdown\n",
    "        \n",
    "        if md_content:\n",
    "            if is_not_found_content(md_content):\n",
    "                print(f\"Skipping saving markdown for {url} because it indicates Not Found.\")\n",
    "            else:\n",
    "                filename = get_safe_filename(url, extension=\".md\")\n",
    "                file_path = os.path.join(output_folder, filename)\n",
    "                with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(md_content)\n",
    "                print(f\"Saved markdown to {file_path}\")\n",
    "        else:\n",
    "            # Fallback to cleaned HTML if no markdown is available.\n",
    "            content = result.cleaned_html or \"\"\n",
    "            if is_not_found_content(md_content):\n",
    "                print(f\"Skipping saving markdown for {url} because it indicates Not Found.\")\n",
    "            else:\n",
    "                filename = get_safe_filename(url, extension=\".html\")\n",
    "                file_path = os.path.join(output_folder, filename)\n",
    "                with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(content)\n",
    "                print(f\"Saved cleaned HTML to {file_path}\")\n",
    "\n",
    "        # Now, get the internal links from the CrawlResult.\n",
    "        # According to the API, result.links is a dictionary that may contain an \"internal\" key.\n",
    "        internal_links = result.links.get(\"internal\", [])\n",
    "        for link_info in internal_links:\n",
    "            href = link_info.get(\"href\")\n",
    "            if href:\n",
    "                # Convert relative URLs to absolute using the current URL as base\n",
    "                absolute_url = resolve_relative_url(href, url)\n",
    "                # Optionally, check that the domain is the same as the original URL\n",
    "                if urlparse(absolute_url).netloc == urlparse(url).netloc:\n",
    "                    await crawl_recursive(crawler, absolute_url, visited, depth + 1, max_depth)\n",
    "    else:\n",
    "        print(f\"Failed to crawl URL: {url}\")\n",
    "        print(f\"Error: {result.error_message}\")\n",
    "\n",
    "# --- Main Recursive Crawling Function ---\n",
    "async def crawl_main(start_urls: List[str], max_depth: int = 2) -> None:\n",
    "    \"\"\"\n",
    "    Main function to start recursive crawling for given start URLs.\n",
    "\n",
    "    Params:\n",
    "        start_urls (List[str]): List of URLs to begin crawling.\n",
    "        max_depth (int):        Maximum allowed recursion depth.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    visited = set()\n",
    "    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n",
    "        for url in start_urls:\n",
    "            await crawl_recursive(crawler, url, visited, depth=0, max_depth=max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_crawl_in_thread() -> None:\n",
    "    \"\"\"Run the recursive crawl in a separate thread.\"\"\"\n",
    "    loop = asyncio.new_event_loop()\n",
    "    asyncio.set_event_loop(loop)\n",
    "    loop.run_until_complete(crawl_main(urls, max_depth=1))\n",
    "    loop.close()\n",
    "\n",
    "t = threading.Thread(target=run_crawl_in_thread)\n",
    "t.start()\n",
    "t.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LightRAG**\n",
    "\n",
    "Setup the ollama embeddings and paramter for the lightRAG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import glob\n",
    "import numpy as np\n",
    "from lightrag import LightRAG, QueryParam\n",
    "from lightrag.llm.ollama import ollama_model_complete, ollama_embedding\n",
    "from lightrag.utils import EmbeddingFunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a wrapper for the embedding function that converts output to np.float32\n",
    "async def my_embedding_func(texts: list[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Async wrapper for ollama_embedding that returns embeddings as a np.float32 array.\n",
    "    \n",
    "    Params:\n",
    "        texts (List[str]): List of input texts to embed.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Embeddings as a float32 NumPy array.\n",
    "    \"\"\"\n",
    "    embeddings = await ollama_embedding(\n",
    "        texts, \n",
    "        embed_model=\"nomic-embed-text\", \n",
    "        host=\"http://localhost:11434\"\n",
    "    )\n",
    "    # Ensure the embeddings are in a NumPy array with dtype float32\n",
    "    return np.array(embeddings, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging level to see info messages\n",
    "logging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.INFO)\n",
    "\n",
    "# Create a working directory where LightRAG will store its cache and index data\n",
    "WORKING_DIR = \"./lightRAG_db\"\n",
    "os.makedirs(WORKING_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize LightRAG with the Ollama model for completion and embedding,\n",
    "# and use FAISS as the vector storage backend.\n",
    "rag = LightRAG(\n",
    "    working_dir=WORKING_DIR,\n",
    "        addon_params={\n",
    "        \"insert_batch_size\": 1             # Process 1 documents per batch\n",
    "    },\n",
    "    llm_model_func=ollama_model_complete,  # Function for generating completions via Ollama \n",
    "    llm_model_name=\"deepseek-r1:1.5b\",     # Specify the Ollama model to use (Gemma 2B in this example) \n",
    "    llm_model_max_async=1,                 # Maximum concurrent requests to the LLM service\n",
    "    llm_model_max_token_size=32768,        # LLM must support at least 32k tokens of context\n",
    "    llm_model_kwargs={\n",
    "        \"host\": \"http://localhost:11434\",       # Ollama service address\n",
    "        \"options\": {\"num_ctx\": 32768,           # Additional options; set context window size\n",
    "                    \"reasoning_tag\": \"think\"},  # For DeepSeek models, set reasoning_tag to \"think\"          \n",
    "    },\n",
    "    embedding_func=EmbeddingFunc(\n",
    "        embedding_dim=768,                # Dimension of the embedding vectors\n",
    "        max_token_size=8192,              # Maximum token size for embedding generation\n",
    "        func=my_embedding_func,           # Function to generate embeddings\n",
    "    ),\n",
    "    vector_storage=\"FaissVectorDBStorage\",  # Use FAISS for vector storage (use faiss-gpu if available)\n",
    "    vector_db_storage_cls_kwargs={\n",
    "        \"cosine_better_than_threshold\": 0.3  # Threshold for retrieval similarity; adjust as needed    \n",
    "    },\n",
    "    chunk_token_size=1000,\n",
    "    chunk_overlap_token_size=100,   \n",
    ")\n",
    "# interesting models for my use case (gpu with 3gb ram): \n",
    "# qwen2.5:1.5b, granite3.1-moe:1b\n",
    "\n",
    "# Rule of thumb: double parameter size of model to get gpu memory requirement: 1.5b -> 3gb, 7b -> 14gb\n",
    "# Dont forget that you need to have head space for the context that goes into the model \n",
    "\n",
    "# deepseek-r1:1.5b - To return only the model's response, you can pass reasoning_tag in llm_model_kwargs.\n",
    "# For example, for DeepSeek models, reasoning_tag should be set to think  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Crawled Documents ---\n",
    "data_folder = \"crawled_data\"\n",
    "md_files = glob.glob(os.path.join(data_folder, \"*.md\"))\n",
    "documents = []\n",
    "for file in md_files:\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read().strip()\n",
    "        # Optionally, skip documents that are empty\n",
    "        if text:\n",
    "            documents.append(text)\n",
    "print(f\"Loaded {len(documents)} documents from {data_folder}\")\n",
    "\n",
    "# --- Insert the Crawled Documents into LightRAG Asynchronously ---\n",
    "await rag.ainsert(documents)\n",
    "print(\"Documents inserted into LightRAG.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Querying the LightRAG System ---\n",
    "# Define a query and test different retrieval modes: \"naive\", \"local\", \"global\", and \"hybrid\".\n",
    "query = \"Please explain why Crawl4AI uses markdown for its output.\"\n",
    "modes = [\"naive\", \"local\", \"global\", \"hybrid\"]\n",
    "\n",
    "for mode in modes:\n",
    "    print(f\"\\nResults using {mode} mode:\")\n",
    "    result = rag.query(query, param=QueryParam(mode=mode))\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

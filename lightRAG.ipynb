{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Website Crawling and Data Extraction\n",
    "\n",
    "In this section, we use `crawl4ai` to crawl one or more websites. The extracted text is saved as `.txt` files in a folder (e.g., `./crawled_data`).\n",
    "\n",
    "You can enter the URLs interactively using the provided widget. If you wish, you can modify the code directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set Windows Proactor Event Loop (for Windows users) ---\n",
    "import sys\n",
    "import asyncio\n",
    "if sys.platform == \"win32\":\n",
    "    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you run into issues with the event loop in Jupyter Notebook, uncomment the next two lines:\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Imports used for Crawl4AI part\n",
    "import asyncio\n",
    "import threading\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "from urllib.parse import urlparse\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "# Import Crawl4AI classes based on the documentation\n",
    "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d18423fbaf40ada8205b61ac084756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='https://example.com\\nhttps://crawl4ai.com/', description='URLs:', layout=Layout(height='80px',â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- URL Input Widget ---\n",
    "url_input = widgets.Textarea(\n",
    "    value='https://example.com\\nhttps://crawl4ai.com/',\n",
    "    placeholder='Enter one URL per line',\n",
    "    description='URLs:',\n",
    "    layout=widgets.Layout(width='100%', height='80px')\n",
    ")\n",
    "display(url_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_safe_filename(url: str, extension: str = \".md\") -> str:\n",
    "    \"\"\"\n",
    "    Generate a safe and unique filename from a URL.\n",
    "    \n",
    "    The filename is based on the domain, path, and (if present) fragment of the URL.\n",
    "    A short hash of the full URL is appended to ensure uniqueness.\n",
    "    \n",
    "    Examples:\n",
    "      - \"https://crawl4ai.com/\" becomes something like \"crawl4ai.com_index_<hash>.md\"\n",
    "      - \"https://crawl4ai.com/#quick-start\" becomes \"crawl4ai.com_index_quick_start_<hash>.md\"\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    \n",
    "    # Get the netloc and sanitize it (replace ':' with '_' if needed)\n",
    "    netloc = parsed.netloc.replace(\":\", \"_\")\n",
    "    \n",
    "    # Use the path; default to \"index\" if empty\n",
    "    path = parsed.path.strip(\"/\")\n",
    "    if not path:\n",
    "        path = \"index\"\n",
    "    \n",
    "    # Use the fragment if present (replace any non-alphanumeric characters with underscores)\n",
    "    fragment = parsed.fragment.strip()\n",
    "    if fragment:\n",
    "        # Keep only alphanumerics and a few safe characters\n",
    "        fragment = \"\".join(c if c.isalnum() or c in \"._-\" else \"_\" for c in fragment)\n",
    "    \n",
    "    # Form the base filename from netloc, path, and fragment (if available)\n",
    "    base = f\"{netloc}_{path}\"\n",
    "    if fragment:\n",
    "        base = f\"{base}_{fragment}\"\n",
    "    \n",
    "    # Sanitize the base further: keep only alphanumerics, dots, underscores, or hyphens\n",
    "    safe_base = \"\".join(c if c.isalnum() or c in \"._-\" else \"_\" for c in base)\n",
    "    \n",
    "    # Compute a short hash of the full URL to ensure uniqueness\n",
    "    hash_digest = hashlib.md5(url.encode(\"utf-8\")).hexdigest()[:8]\n",
    "    \n",
    "    # Combine safe_base, hash, and extension\n",
    "    safe_filename = f\"{safe_base}_{hash_digest}{extension}\"\n",
    "    return safe_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_content(content: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove wrapping triple backticks (```) if present,\n",
    "    as well as any language hints (like \"json\") that might appear\n",
    "    immediately after the first set of backticks.\n",
    "    \"\"\"\n",
    "    stripped = content.strip()\n",
    "    # Remove any leading/trailing triple backticks\n",
    "    # This pattern removes the first line if it starts with ``` (optionally followed by a language)\n",
    "    # and the last line if it ends with ```\n",
    "    if stripped.startswith(\"```\") and stripped.endswith(\"```\"):\n",
    "        # Remove the first and last lines\n",
    "        lines = stripped.splitlines()\n",
    "        # If the first line is only backticks (or backticks with a language hint)\n",
    "        if lines[0].strip().startswith(\"```\"):\n",
    "            lines = lines[1:]\n",
    "        if lines and lines[-1].strip().endswith(\"```\"):\n",
    "            lines = lines[:-1]\n",
    "        normalized = \"\\n\".join(lines)\n",
    "        return normalized.strip()\n",
    "    return content\n",
    "\n",
    "def is_not_found_content(content: str) -> bool:\n",
    "    \"\"\"\n",
    "    Return True if content can be parsed as JSON and equals {\"detail\": \"Not Found\"}\n",
    "    (ignoring extra whitespace or differences in spacing).\n",
    "    \"\"\"\n",
    "    normalized = normalize_content(content)\n",
    "    try:\n",
    "        data = json.loads(normalized)\n",
    "        # Check that it's a dict with a key \"detail\" whose value (case-insensitive) is \"not found\"\n",
    "        if isinstance(data, dict) and data.get(\"detail\", \"\").strip().lower() == \"not found\":\n",
    "            return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of URLs to crawl: ['https://crawl4ai.com/']"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Convert Widget Content to a URL List ---\n",
    "urls = url_input.value.strip().splitlines()\n",
    "print(\"List of URLs to crawl:\", urls)\n",
    "\n",
    "# --- Set Up Browser Configuration ---\n",
    "browser_cfg = BrowserConfig(\n",
    "    browser_type=\"chromium\",\n",
    "    headless=True,\n",
    "    verbose=True,\n",
    "    viewport_width=1280,\n",
    "    viewport_height=720,\n",
    ")\n",
    "\n",
    "# CrawlerRunConfig is set up to:\n",
    "# - Exclude external links (only internal links remain in result.links)\n",
    "run_cfg = CrawlerRunConfig(\n",
    "    cache_mode=CacheMode.BYPASS,\n",
    "    word_count_threshold=15,\n",
    "    exclude_external_links=True,\n",
    "    stream=True,  # For efficient processing when using arun_many()/arun()\n",
    ")\n",
    "\n",
    "# --- Create output folder for crawled data ---\n",
    "output_folder = \"crawled_data\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# --- Define the Recursive Crawling Function ---\n",
    "async def crawl_recursive(crawler, url, visited, depth, max_depth):\n",
    "    if depth > max_depth or url in visited:\n",
    "        return\n",
    "    visited.add(url)\n",
    "    \n",
    "    # Perform the crawl for a single URL\n",
    "    result = await crawler.arun(url, config=run_cfg)\n",
    "    if result.success:\n",
    "        print(f\"Crawled URL: {result.url} at depth {depth}\")\n",
    "        \n",
    "        # Attempt to use markdown output (preferably via markdown_v2 or markdown field)\n",
    "        md_content = None\n",
    "        if result.markdown:\n",
    "            if isinstance(result.markdown, str):\n",
    "                md_content = result.markdown\n",
    "            elif hasattr(result.markdown, \"raw_markdown\"):\n",
    "                md_content = result.markdown.raw_markdown\n",
    "        elif result.markdown_v2 and hasattr(result.markdown_v2, \"raw_markdown\"):\n",
    "            md_content = result.markdown_v2.raw_markdown\n",
    "        \n",
    "        if md_content:\n",
    "            if is_not_found_content(md_content):\n",
    "                print(f\"Skipping saving markdown for {url} because it indicates Not Found.\")\n",
    "            else:\n",
    "                filename = get_safe_filename(url, extension=\".md\")\n",
    "                file_path = os.path.join(output_folder, filename)\n",
    "                with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(md_content)\n",
    "                print(f\"Saved markdown to {file_path}\")\n",
    "        else:\n",
    "            # Fallback to cleaned HTML if no markdown is available.\n",
    "            content = result.cleaned_html or \"\"\n",
    "            if is_not_found_content(md_content):\n",
    "                print(f\"Skipping saving markdown for {url} because it indicates Not Found.\")\n",
    "            else:\n",
    "                filename = get_safe_filename(url, extension=\".html\")\n",
    "                file_path = os.path.join(output_folder, filename)\n",
    "                with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(content)\n",
    "                print(f\"Saved cleaned HTML to {file_path}\")\n",
    "\n",
    "        # Now, get the internal links from the CrawlResult.\n",
    "        # According to the API, result.links is a dictionary that may contain an \"internal\" key.\n",
    "        internal_links = result.links.get(\"internal\", [])\n",
    "        for link_info in internal_links:\n",
    "            href = link_info.get(\"href\")\n",
    "            if href:\n",
    "                # Convert relative URLs to absolute using the current URL as base\n",
    "                absolute_url = urljoin(url, href)\n",
    "                # Optionally, check that the domain is the same as the original URL\n",
    "                if urlparse(absolute_url).netloc == urlparse(url).netloc:\n",
    "                    await crawl_recursive(crawler, absolute_url, visited, depth + 1, max_depth)\n",
    "    else:\n",
    "        print(f\"Failed to crawl URL: {url}\")\n",
    "        print(f\"Error: {result.error_message}\")\n",
    "\n",
    "# --- Main Recursive Crawling Function ---\n",
    "async def crawl_main(start_urls, max_depth=2):\n",
    "    visited = set()\n",
    "    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n",
    "        for url in start_urls:\n",
    "            await crawl_recursive(crawler, url, visited, depth=0, max_depth=max_depth)\n",
    "            \n",
    "    # --- Run the Recursive Crawler ---\n",
    "# await crawl_main(urls, max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_crawl_in_thread():\n",
    "    asyncio.run(crawl_main(urls, max_depth=4))\n",
    "\n",
    "t = threading.Thread(target=run_crawl_in_thread)\n",
    "t.start()\n",
    "t.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
